---
title: "Introdução à álgebra linear com R"
author: "Kaio Vital da Costa"
date: "20/03/2020"
output: 
  html_document:
    theme: readable
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include=FALSE }
library(htmlwidgets)
library(knitr)
library(shiny)
library(matlib)
library(bookdown)
library(bibtex)
library(vembedr)
library(htmltools)
knitr::opts_chunk$set(echo = TRUE)
```

    Não é um curso de álgebra linear

    Requer conhecimento básico em álgebra linear

    O curso é voltado para aplicações dos principais conceitos utilizados em análises baseadas em matrizes de insumo-produto

Alguns pacotes interessantes para tratamento de matrizes e sistemas lineares:

* [matrixcalc](https://cran.r-project.org/web/packages/matrixcalc/matrixcalc.pdf)
* [matrix](https://cran.r-project.org/web/packages/Matrix/Matrix.pdf)
* [limSolve](https://cran.r-project.org/web/packages/limSolve/vignettes/limSolve.pdf)
* [matlib](https://cran.r-project.org/web/packages/matlib/vignettes/linear-equations.htm)
* [blockmatrix](https://cran.r-project.org/web/packages/blockmatrix/blockmatrix.pdf)
* [Lista dos principais operadores](http://www.statmethods.net/advstats/matrix.html )


## Introdução álgebra linear com R {.tabset}

### Pacotes necessários

Para rodar os códigos abaixo você irá precisar de alguns pacotes instalados. Se o pacote não estiver presente instale-os usando a função `install.packages()`, com o nome do pacote entre `""`.

```{r}
library(UsingR)
library(matlib)
library(vembedr)
```


### Principais conceitos 

1. **Notação matricial**

Aqui, apresentamos o básico da notação matricial. Inicialmente, isso pode parecer complicado demais, mas uma vez discutidos os exemplos, você conhecerá o potencial de usar essa notação para explicar e derivar soluções, além de implementá-las como código R.

1.1 **A linguagem dos modelos lineares**

A notação de álgebra linear na verdade simplifica as descrições matemáticas e manipulações de modelos lineares, bem como a codificação em R. Discutiremos o básico dessa notação e depois mostraremos alguns exemplos em R.

O ponto principal de todo este exercício é mostrar como podemos escrever alguns modelos usando a notação matricial e, em seguida, explicar como isso é útil para análises baseadas em matrizes insumo-produto. Começamos simplesmente definindo a notação e a multiplicação de matrizes, mas eventualmente voltamos à aplicação prática.

1.2 **Resolvendo sistemas de equações lineares**

A álgebra linear foi criada por matemáticos para resolver sistemas de equações lineares como esta:

$$
\begin{align*}
a + b + c &= 6\\
3a - 2b + c &= 2\\
2a + b  - c &= 1
\end{align*}
$$
A álgebra linear fornece instrumentais muito úteis para resolver esses problemas em geral. Vamos aprender como podemos escrever e resolver esse sistema usando a notação de álgebra matricial:

$$ 
\,
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix} =
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
\implies
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix} =
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}^{-1}
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
$$
Esta seção explica a notação usada acima. Também podemos emprestar essa notação para modelos lineares em estatística.

1.3 **Vetores, Matrizes, and escalares**

No exemplo abaixo, as variáveis aleatórias associadas aos dados foram representadas por $Y_1,\dots,Y_n$. Podemos pensar nisso como um vetor. De fato, no R já estamos fazendo isso:

```{r,message=FALSE}
data(father.son,package="UsingR")
y=father.son$fheight
head(y)
```
Em matemática, também podemos usar apenas um símbolo. Geralmente, usamos negrito para diferenciá-lo das entradas individuais:

$$ \mathbf{Y} = \begin{pmatrix}
Y_1\\\
Y_2\\\
\vdots\\\
Y_N
\end{pmatrix}
$$

Por razões que em breve ficarão claras, a representação padrão dos vetores por colunas tem a dimensão $N\times 1$ em oposição aos vetores por linhas $1 \times N$.

Aqui nem sempre usamos negrito, porque normalmente se pode dizer o que é uma matriz a partir do contexto. Embora no principal livro para estudos em matrizes insumo-produto a notação mude relativamente ao que utilizamos aqui.

Da mesma forma, podemos usar notação matemática para representar as covariáveis ou preditores. Em um caso com dois preditores, podemos representá-los assim:

$$ 
\mathbf{X}_1 = \begin{pmatrix}
x_{1,1}\\
\vdots\\
x_{N,1}
\end{pmatrix} \mbox{ and }
\mathbf{X}_2 = \begin{pmatrix}
x_{1,2}\\
\vdots\\
x_{N,2}
\end{pmatrix}
$$

Em nosso exemplo acima (podendo ser um objeto em queda) $x_{1,1}=t_i$ e $x_{i,1}=t_i^2$ com $t_i$ o tempo da i-ésima observação. Além disso, lembre-se de que vetores podem ser considerados matrizes $N \times 1$.

Por razões que em breve se tornarão mais óbvias, é conveniente representá-las em matrizes:

$$ 
\mathbf{X} = [ \mathbf{X}_1 \mathbf{X}_2 ] = \begin{pmatrix}
x_{1,1}&x_{1,2}\\
\vdots\\
x_{N,1}&x_{N,2}
\end{pmatrix}
$$

Essa matriz tem dimensão $N\times 2$. Podemos criar essa matriz no R desta maneira:

```{r}
n <- 25
tt <- seq(0,3.4,len=n) 
X <- cbind(X1=tt,X2=tt^2)
head(X)
dim(X)
```

Também podemos usar essa notação para indicar um número arbitrário de covariáveis com a seguinte matriz $N \times p$:

$$
\mathbf{X} = \begin{pmatrix}
  x_{1,1}&\dots & x_{1,p} \\
  x_{2,1}&\dots & x_{2,p} \\
   & \vdots & \\
  x_{N,1}&\dots & x_{N,p} 
  \end{pmatrix}
$$

Apenas como exemplo, mostramos como fazer no R agora usando `matrix` em vez de` cbind`:

```{r}
N <- 100; p <- 5
X <- matrix(1:(N*p),N,p)
head(X)
dim(X)
```

Por padrão, as matrizes são preenchidas coluna por coluna. O argumento `byrow = TRUE` permite alterar isso para linha por linha:

```{r}
N <- 100; p <- 5
X <- matrix(1:(N*p),N,p,byrow=TRUE)
head(X)
```

Finalmente, definimos um escalar. Um escalar é apenas um número, que chamamos de escalar porque queremos distingui-lo de vetores e matrizes. Geralmente usamos letras minúsculas e não negrito. Agora entenderemos por que fazemos essa distinção.

2. **Operações matriciais**

Na seção anterior,iniciamos nossos estudos de introdução à álgebra linear com este sistema de equações:

$$
\begin{align*}
a + b + c &= 6\\
3a - 2b + c &= 2\\
2a + b  - c &= 1
\end{align*}
$$

Descrevemos como esse sistema pode ser reescrito e resolvido usando álgebra matricial:

$$
\,
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix} =
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
\implies
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix}=
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}^{-1}
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
$$

Uma vez descrita a notação matricial, explicaremos a operação que realizamos com eles. Por exemplo, acima, temos multiplicação de matrizes e também temos um símbolo que representa a inversa de uma matriz. A importância dessas operações e outras ficará clara quando apresentarmos exemplos específicos relacionados à análise de dados.

2.1 **Multiplicação por um escalar**

Começamos com uma das operações mais simples: multiplicação escalar. Se $a$ é um escalar e $\mathbf{X}$ for uma matriz, então:

$$
\mathbf{X} = \begin{pmatrix}
  x_{1,1}&\dots & x_{1,p} \\
  x_{2,1}&\dots & x_{2,p} \\
   & \vdots & \\
  x_{N,1}&\dots & x_{N,p} 
  \end{pmatrix} \implies
a \mathbf{X} = 
\begin{pmatrix}
  a x_{1,1} & \dots & a x_{1,p}\\
  a x_{2,1}&\dots & a x_{2,p} \\
  & \vdots & \\
  a x_{N,1} & \dots & a  x_{N,p}
\end{pmatrix}
$$

R segue automaticamente esta regra quando multiplicamos um número por uma matriz usando `*`:

```{r}
X <- matrix(1:12,4,3)
print(X)
a <- 2
print(a*X)
```

2.2 **A transposta (vetor e/ou vetor)**

A transposta é uma operação que simplesmente altera colunas para linhas. Usamos $\top$ para denotar uma transposição, Mas é possível encontrar em alguns livros voltados ao estudo de matrizes insumo-produto pelo sinal de *'* A definição técnica é a seguinte: se X é como definimos acima, aqui está a transposta que será $p \times N$:

$$
\mathbf{X} = \begin{pmatrix}
  x_{1,1}&\dots & x_{1,p} \\
  x_{2,1}&\dots & x_{2,p} \\
   & \vdots & \\
  x_{N,1}&\dots & x_{N,p} 
  \end{pmatrix} \implies
\mathbf{X}^\top = \begin{pmatrix}
  x_{1,1}&\dots & x_{p,1} \\
  x_{1,2}&\dots & x_{p,2} \\
   & \vdots & \\
  x_{1,N}&\dots & x_{p,N} 
  \end{pmatrix}
$$

No R utilizamos apenas `t`:

```{r}
X <- matrix(1:12,4,3)
X
t(X)
```
2.3 **Multiplicação de matrizes**

Começamos descrevendo a multiplicação de matrizes mostrada no exemplo original do sistema de equações:

$$
\begin{align*}
a + b + c &=6\\
3a - 2b + c &= 2\\
2a + b  - c &= 1
\end{align*}
$$
O que estamos fazendo é multiplicar as linhas da primeira matriz pelas colunas da segunda. Como a segunda matriz possui apenas uma coluna, realizamos essa multiplicação da seguinte maneira:

$$
\,
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix}=
\begin{pmatrix}
a + b + c \\
3a - 2b + c \\
2a + b  - c 
\end{pmatrix}
$$
Aqui está um exemplo simples. Podemos verificar se `abc = c (3,2,1)` é uma solução:

```{r}
X  <- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)
abc <- c(3,2,1) #use como um simples exemplo
rbind( sum(X[1,]*abc), sum(X[2,]*abc), sum(X[3,]*abc))
```

Podemos usar o `% *%` para executar a multiplicação da matriz e tornar isso muito mais compacto:

```{r}
X%*%abc
```

Podemos ver que `c (3,2,1)` não é uma solução, pois a resposta aqui não é a necessária `c (6,2,1)`.

Para obter a solução, precisaremos inverter a matriz à esquerda, um conceito que aprenderemos agora:

Aqui está a definição geral da multiplicação de matrizes $A$ e $X$:

$$
\mathbf{AX} = \begin{pmatrix}
  a_{1,1} & a_{1,2} & \dots & a_{1,N}\\
  a_{2,1} & a_{2,2} & \dots & a_{2,N}\\
  & & \vdots & \\
  a_{M,1} & a_{M,2} & \dots & a_{M,N}
\end{pmatrix}
\begin{pmatrix}
  x_{1,1}&\dots & x_{1,p} \\
  x_{2,1}&\dots & x_{2,p} \\
   & \vdots & \\
  x_{N,1}&\dots & x_{N,p} 
  \end{pmatrix}
$$
$$  = \begin{pmatrix}
  \sum_{i=1}^N a_{1,i} x_{i,1} & \dots & \sum_{i=1}^N a_{1,i} x_{i,p}\\
  & \vdots & \\
  \sum_{i=1}^N a_{M,i} x_{i,1} & \dots & \sum_{i=1}^N a_{M,i} x_{i,p}
\end{pmatrix}
$$
Você só pode usar o produto se o número de colunas da primeira matriz $A$ for igual ao número de linhas da segunda $X$. Além disso, a matriz final possui os mesmos números de linhas do primeiro $A$ e os mesmos números de colun do segundo $X$.
Depois de estudar o exemplo abaixo, você pode voltar e reler as seções acima.

2.4 **A matrix identidade**

A matriz identidade é análoga ao número 1: se você multiplicar a matriz identidade por outra matriz, obtém a mesma matriz. Para que isso aconteça, precisamos que:

$$
\mathbf{I} = \begin{pmatrix}
1&0&0&\dots&0&0\\
0&1&0&\dots&0&0\\
0&0&1&\dots&0&0\\
\vdots &\vdots & \vdots&\ddots&\vdots&\vdots\\
0&0&0&\dots&1&0\\
0&0&0&\dots&0&1
\end{pmatrix}
$$

Por essa definição, a identidade sempre deve ter o mesmo número de linhas que as colunas ou ser o que chamamos de matriz quadrada.

Se você seguir a regra de multiplicação de matrizes que apresentamos acima, notará que isso funciona:

$$
\mathbf{XI} = 
\begin{pmatrix}
   x_{1,1} & \dots &  x_{1,p}\\
  & \vdots & \\
   x_{N,1} & \dots &   x_{N,p}
\end{pmatrix}
\begin{pmatrix}
1&0&0&\dots&0&0\\
0&1&0&\dots&0&0\\
0&0&1&\dots&0&0\\
 & & &\vdots& &\\
0&0&0&\dots&1&0\\
0&0&0&\dots&0&1
\end{pmatrix} = 
\begin{pmatrix}
   x_{1,1} & \dots &  x_{1,p}\\
  & \vdots & \\
   x_{N,1} & \dots & x_{N,p}
\end{pmatrix}
$$

No R, você pode formar uma matriz identidade da seguinte maneira:

```{r}
n <- 5 #pick dimensions
diag(n)
```

3. **Matrizes quadradas e determinantes**

A seção a seguir ilustra como uma matriz pode ser usada para representar um sistema de equações lineares.

3.1 Matrizes quadradas

3.1.1 Matriz identidade

Como uma forma de matriz muito especial, podemos definir a matriz de identidade, que será usada em muitas operações de matriz. De fato, já definimos a matriz  identidade, mas agora fazemos de forma geral. Antes disso, vamos primeiro introduzir a função [delta de Kronecker](https://pt.wikipedia.org/wiki/Delta_de_Kronecker):

\begin{equation}
  \delta_{ij}=\begin{cases}
    1, & \text{se $i=j$}\\
    0, & \text{caso contrário}.
  \end{cases}
\end{equation}

A matriz identidade pode, então, ser definida como

<div align="center">$I_{n}=||\delta_{ij}||_{nxn}$</div>

ou

$$
\mathbf{I_n} = \begin{pmatrix}
1&0&0\\
0&1&0\\
0&0&1\\
\end{pmatrix}
$$

Sendo $A=||a_ij||_{nxn}$, então $AI_{n}=I_{n}A=A$.

3.1.2 Poder de uma matriz e polinômio de uma matriz

Seja **$A$** uma matriz quadrada de ordem $m$. Podemos definir a potência de qualquer matriz quadrada **$A$** da seguinte maneira

$$
\begin{align*}
A^{0}=I_{n}\\
A^{1}=A\\
A^{2}=AA\\
.........\\
A^{n}=AA^{n-1}
\end{align*}
$$
O polinômio usual é dado por $f(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+...+a_{1}x+a_{0}$. O polinômio da matriz **$A$** pode ser definido similarmente como

$$
\begin{align*}
f(A)=a_{n}A^{n}+a_{n-1}x^{n-1}+...+a_{1}A+a_{0}I_{m}.
\end{align*}
$$
3.1.3 Sistema de equações lineares: o caso de duas variáveis

Considere um sistema arbitrário de duas equações lineares com duas variáveis

\begin{equation}
\tag{1}
  \begin{cases}
    a_{11}x_{1}+a_{22}x_{2}=b_{1} \\
    a_{21}x_{1}+a_{22}x_{2}=b_{2}.
  \end{cases}
\end{equation}

Para o sistema acima, deixe

$$
\mathbf{A} = \begin{pmatrix}
  a_{11}&a_{12}\\
  a_{21}&x_{22}\\
    \end{pmatrix}_{2x2} 
$$  
ser a matriz dos coeficientes conhecidos,

$$x=\begin{bmatrix}
x_{1}\\
x_{2}\\
\end{bmatrix}_{2x1}
$$
seja o vetor de variáveis desconhecidas e

$$b=\begin{bmatrix}
b_{1}\\
b_{2}\\
\end{bmatrix}_{2x1}
$$
seja o vetor de constantes. Portando, na forma matricial podemos reescrever nosso sistema de equações lineares como

$$
\tag{2} 
\begin{align*} 
\mathbf{A}x=b
\end{align*}
$$
Agora, vamos tentar resolver o sistema acima no caso "genérico", desde que nenhuma divisão por zero apareça em nossos cálculos. Vamos resolvê-lo com a exclusão de variáveis desconhecidas.
Divida a primeira linha por $a_{11}$ e a segunda linha $a_{21}$, assumindo que ambos sejam diferentes de zero.

\begin{equation}
  \begin{cases}
    x_{1}+(\frac{a_{12}}{a_{11}})x_{2}=\frac{b_{1}}{a_{11}} \\
    x_{1}+(\frac{a_{22}}{a_{21}})x_{2}=\frac{b_{2}}{a_{21}}.
  \end{cases}
\end{equation}

Subtraia a segunda linha da primeira.

\begin{equation}
  (\frac{a_{12}}{a_{11}}-\frac{a_{22}}{a_{21}})x_{2}=\frac{b_{1}}{a_{11}}-\frac{b_{2}}{a_{21}}
 \end{equation}

E então $x_{2}$ é obtido

\begin{equation}
\tag{3}
x_{2}=(\frac{b_{1}}{a_{11}})-(\frac{b_{2}}{a_{21}})/(\frac{a_{12}}{a_{11}})-(\frac{a_{22}}{a_{21}}) \\
     ={b_{2}}{a_{11}}-{b_{1}}{a_{21}}/{a_{11}}{a_{22}}-{a_{12}}{a_{21}}
\end{equation}

Inserindo o valor de $x_{2}$ na equação original e resolvendo para $x_{1}$, temos:

\begin{equation}
\tag{4}
x_{1}={b_{1}}{a_{22}}-{b_{2}}{a_{12}}/{a_{11}}{a_{22}}-{a_{12}}{a_{21}}
\end{equation}

No R, usaremos o pacote `matlib`, logo devemos chamar `library(matlib)`:

```{r}
A <- matrix(c(1, 2, -1, 2), 2, 2)
b <- c(2,1)
showEqn(A, b)
c( R(A), R(cbind(A,b)) )          # mostre os ranks
all.equal( R(A), R(cbind(A,b)) )  # consistente?
```
Graficamente:
```{r, echo=2}
par(mar=c(4,4,0,0)+.1)
plotEqn(A,b)
```
`Solve ()` é uma função muito conveniente que mostra a solução de uma forma mais compreensível:
```{r}
Solve(A, b, fractions = TRUE)
```

3.1.4 O determinante de uma matriz

Para a matriz de coeficientes 

$$
\mathbf{A} = \begin{pmatrix}
  a_{11}&a_{12}\\
  a_{21}&x_{22}\\
    \end{pmatrix}
$$ 
podemos definir a função 

$$
\begin{align*}
det(A)=a_{11}a_{22}-a_{21}a_{12}.
\end{align*}
$$
chamado *determinante* da matriz **$A$**. É igual aos denominadores das frações no lado direito da fórmula (3) e (4) para a solução do sistema (1). Então os numeradores dessas frações são iguais aos determinantes de outras matrizes, $b_{1}a_{22}-b_{2}a_{12}=det(A)'$ e $b_{2}a_{11}-b_{1}a_{21}=det(A)''$.Onde

$$
\mathbf{A'} = \begin{pmatrix}
  b_{1}&a_{12}\\
  a_{2}&a_{22}\\
    \end{pmatrix}
$$ 
e

$$
\mathbf{A''} = \begin{pmatrix}
  a_{11}&b_{1}\\
  a_{12}&b_{2}\\
    \end{pmatrix}.
$$ 
Então, podemos reescrever $x_{1}$ e $x_{2}$ como

\begin{equation}
\tag {5}
x_{1}=\frac{det(A')}{det(A)}\\
x_{2}=\frac{det(A'')}{det(A)}
\end{equation}

Agora vamos estudar as propriedades da função

$$
\mathbf{det} \begin{pmatrix}
  a_{11}&a_{12}\\
  a_{21}&a_{22}\\
    \end{pmatrix}.
$$
Podemos denotar a matriz **$A_{2x2}$** como um par de suas duas colunas $A_{1} = \begin{bmatrix}a_{11}\\ a_{21} \end{bmatrix}$ e $A_{2} = \begin{bmatrix}a_{12}\\ a_{22} \end{bmatrix}$, isto é, $A = \begin{bmatrix}A_{1},\ A_{2} \end{bmatrix}$. Então, temos que:
Linearidade
a. $det[A_{1}+A'_{1},A_{2}]=det[A_{1},A_{2}]+det[A'_{1},A_{2}].$

b. $det[A_{1},A_{2}+A'_{2}]=det[A_{1},A_{2}]+det[A_{1},A'_{2}].$

c. $det[cA_{1},A_{2}]=cdet[A_{1},A_{2}].$

d. $det[A_{1},cA_{2}]=cdet[A_{1},A_{2}].$

Anti-simetria
e. $det[A_{1},A_{2}]=-det[A_{2},A_{1}].$

f. $det[A_{1},A_{1}]=0.$

g. $detI_{2}=1.$

O determinante de uma matriz $n\times n$ é definida como segue.

Deixe $A=[a_{ij}]_{n\times n}$. O *menor* $M_{ij}$ de $A$ é o determinante da matriz formada a partir de $A$ removendo a i-ésima linha e a j-ésima coluna, enquanto o produto $A_{ij}=(-1)^{i+j}M_{ij}$ é chamado um cofator do elemento $a_{ij}.$

Então em termos da linha $i$ e os respectivos cofatores, o detemrinante de $A$ é

$$
\tag {6}
\begin{align*}
det(A)=\sum_{j=1}^na_{ij}A_{ij}.
\end{align*}
$$
ou em termos dos elementos da coluna $j$ e seus cofatores, temos

$$
\tag {7}
\begin{align*}
det(A)=\sum_{i=1}^na_{ij}A_{ij}.
\end{align*}
$$
**Propriedades do determinante** 

1. O determinante de uma matriz $A$ é igual ao determinante da sua transposta, $det(A)=det(A^\top)$;

2. Se todos os elementos de uma linha ou coluna forem iguais a zero, o determinante será nulo;

3. Multiplicando-se uma linha (ou coluna) de uma matriz $A$ por um escalar $k \in \mathbb{R}^2$, o determinante da nova matriz $B$ é igual ao produto do escalar pelo determinante da matriz $A$;

4. Permutadas duas linhas, o determinante troca de sinal;

5. Uma matriz com duas linhas (colunas) iguais possui determinante igual a zero. Idem se existem linhas (colunas) que são múltiplos das outras.

6. O determinante de uma matriz triangular superior (ou inferior) $A$ é igual ao produto dos elementos da diagonal principal, ou seja, $det(A)=a_{11}a_{22}\cdot \cdot \cdot a_{nn}$ (expansão por Laplace).

No R

```{r}
A <- matrix(c(6, 2, 3, 2, 4, 2, 1, 1, 8), 3, 3)
det(A)
det(t(A))
```

```{r}
F <- A
F[1,] <- rep(0,3) #det(F) será nulo
det(F)
```
```{r}
H <- A[, c(2, 1, 3)] # permuta de linha/coluna
det(H)
```

```{r fig.width=3, fig.height=3}
library(matlib)
xlim <- c(0,8)
ylim <- c(0,8)
par(mar=c(3,3,1,1)+.1)
plot(xlim, ylim, type="n", xlab="X1", ylab="X2", asp=1)
sum <- A[1,] + A[2,]
# desenha o paralelograma determinado pelas linhas de A
polygon( rbind(c(0,0), A[1,], sum, A[2,]), col=rgb(1,0,0,.2))
vectors(A, labels=c("a1", "a2"), pos.lab=c(4,2))
vectors(sum, origin=A[1,], col="gray")
vectors(sum, origin=A[2,], col="gray")
# adiciono algumas anotações
text(0,6, "det(A) é a área de seus vetores linha", pos=4)
text(mean(A[,1]), mean(A[,2]), "det(A)", cex = 0.7)
```

Finalmente, também podemos ver por que o determinante é zero quando as linhas ou colunas são proporcionais.

```{r}
(B <- matrix(c(1, 2, 2, 4), 2,2))
det(B)
```
Esses vetores são chamados *colineares*. Eles não incluem *área*.
```{r fig.width=4, fig.height=4}
par(mar=c(3,3,1,1)+.1)
plot(c(0,4), c(0,4), type="n", xlab="X1", ylab="X2", asp=1)
vectors(B, labels=c("b1", "b2"), pos.lab=c(4,2))
```

3.1.4.1 Calculando o `det()` por expansão de cofator

Defina uma matriz $3 \times 3$ e encontre seu determinante.

```{r }
  A <- matrix(c(4, 2, 1,
                5, 6, 7,
                1, 0, 3), nrow=3, byrow=TRUE)
  det(A)
```

Encontre cofatores dos elementos da linha 1^[No R, subscritos negativos deletam linhas ou colunas]:

O cofator $A_{i,j}$ do elemento $a_{i,j}$ é o determinante resultante do que resta quando a linha $i$, coluna $j$ da matriz $A$ é excluída.

```{r }
  cat(cofactor(A, 1, 1),  "  ==  ",  1 * det((A[-1, -1]), "\n"))
  cat(cofactor(A, 1, 2),  "  ==  ", -1 * det((A[-1, -2]), "\n"))
  cat(cofactor(A, 1, 3),  "  ==  ",  1 * det((A[-1, -3]), "\n"))
```

O determinante é o produto das linhas com os cofatores ou matematicamente: $\det(A) = a_{1,1} * A_{1,1} + a_{1,2} * A_{1,2} + a_{1,3} * A_{1,3}$

`rowCofactors ()` é uma função muito útil, que calcula tudo isso junto

```{r }
  rowCofactors(A, 1)
```

```{r }
  A[1,] %*% rowCofactors(A, 1)
  all.equal(det(A), c(A[1,] %*% rowCofactors(A, 1)))
```

3.1.5 **A inversa**

A inversa da matriz $X$, denotado com $X^{- 1}$, possui a propriedade que, quando multiplicada, fornece a identidade $X^{- 1}X=I$. Obviamente, nem todas as matrizes têm inversas. Por exemplo, uma matriz $2 \times 2$ com 1s em todas as suas entradas não tem uma inversa.

Como veremos quando chegarmos às seções dedicadas às aplicações de álgebra linear ao estudo das matrizes insumo-produto, poder calcular a inversa de uma matriz é bastante útil, possuindo importantes implicações na economia. Um aspecto muito conveniente do R é que ele inclui uma função predefinida, `solve`, para fazer isso. Aqui está como a usaríamos para resolver o linear das equações:

```{r}
X <- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)
y <- matrix(c(6,2,1),3,1)
solve(X)%*%y #equivalente a solve(X,y)
```

A inversa de uma matriz desempenha os mesmos papéis na álgebra matricial que o recíproco de um número e divisão na aritmética comum:  assim como podemos resolver uma equação simples como $4x=8$ for $x$ por multiplicar ambos os lados pelo recíproco $$4x=8 \Rightarrow 4^{-1} 4x=4^{-1}8 \Rightarrow x=8/4=2$$
podemos resolver uma equações matricial como $\mathbf{Ax} = \mathbf{b}$ para o vetor $\mathbf{x}$ por multiplicar ambos os lados pela inversa da matriz $\mathbf{A}$, $$\mathbf{Ax}= \mathbf{b} \Rightarrow \mathbf{A}^{-1} \mathbf{Ax} = \mathbf{A}^{-1} \mathbf{b} \Rightarrow \mathbf{x} = \mathbf{A}^{-1} \mathbf{b}$$




### O modelo de Leontief

**1. Modelo linear de produção em uma abordagem clássica**

Na economia clássica, as interdependências setoriais na estrutura produtiva são vitais para entender as leis de produção e distribuição e, portanto, para entender como funciona um sistema econômico. Wassily Leontief^[Wassily Leontief (1906–1999) nasceu e foi educado em São Petersburgo. Ele recebeu seu Ph. D. pela Universidade de Berlim em 1929. Ele entrou para o famoso Instituto Kiel de Economia Mundial em 1927 e trabalhou lá até 1930. Ele se mudou para os EUA em 1931; trabalhou na Universidade de Harvard (1932–1975) e na Universidade de Nova York (1975–1991). Leontief recebeu o Prêmio Nobel de Economia em 1973 por sua contribuição à análise de entrada e saída.], economista americano nascido na Rússia, deu a maior contribuição nessa linha de pensamento, desenvolvendo a análise de insumo-produto.

Para explorar essa idéia de maneira simples, considere uma economia que consiste em três atividades: mineração de carvão, geração de eletricidade e produção de caminhões. Os resultados dessas atividades são utilizados na produção de outros e também são exigidos para uso final, ou seja, consumo e / ou investimento. Por exemplo, extrair carvão requer energia. Parte de sua necessidade de energia pode ser satisfeita internamente, usando carvão para gerar eletricidade. O restante é obtido da usina. A mina de carvão também emprega vários caminhões para entregar sua produção. Nesse cenário, esta planta está usando carvão, energia e caminhões para produzir carvão. Podemos expressar essa relação da seguinte maneira 

$$\begin{array}{ccc}
\tag{1}
(carvão, eletricidade, caminhões)\implies carvão
\end{array}
$$
A produção de carvão é usada por outros setores. Parte dela, como foi indicado acima, usada pela mina de carvão para produzir energia para seu próprio uso. As unidades térmicas da usina podem estar usando carvão para produzir energia. No entanto, a produção de caminhões pode não exigir carvão como insumo. Portanto, a produção restante é usada para satisfazer o consumo final das famílias para aquecimento. Para essas atividades, a relação acima pode ser escrita como

$$\begin{array}{ccc}
\tag{2}
(carvão, eletricidade, caminhões)\implies eletricidade
\end{array}
$$
$$\begin{array}{ccc}
\tag{3}
(eletricidade, caminhões)\implies caminhões
\end{array}
$$
Por outro lado, assumindo que a oferta de carvão é igual à sua demanda, podemos escrever $$\tag {4} \text{produção de carvão = carvão utilizado nas minas + carvão utilizado nos setores + carvão utilizado pelas famílias.}$$

As demais atividades de produção podem ser definidas de maneira semelhante. A energia elétrica é usada para operar a usina, bem como em outras atividades de produção. As famílias também exigem eletricidade para iluminação e/ou aquecimento. Caminhões podem ser usados para todas as atividades comerciais. No entanto, podemos pensar que eles podem não ser adequados como carros familiares e, portanto, não são exigidos para consumo final. Portanto, podemos escrever 

$$\begin{align*}
\tag{5}
\text{produção de eletricidade} = \text{eletricidade utilizada nas minas} \\+ \text{eletricidade utilizada no próprio setor} \\+ \text{eletricidade utilizada no setor de caminhões} \\+ \text{consumo de eletricidade das famílias}.
\end{align*}
$$
$$\begin{align*}
\tag{6}
\text{produção de caminhões} = \text{caminhões utilizados nas minas} \\+ \text{caminhões utilizados no setor de eletricidade} \\+ \text{caminhões utilizados no próprio setor}.
\end{align*}
$$
Deixe $X_{ij}$ denotar a quantidade do bem $i$ demandada pelo setor $j$ e $Y_{i}$ a demanda final das famílias. Então, 4-6 podem ser escritas como

\begin{equation}
\tag{7}
  \begin{cases}
    X_{1}=X_{11}+X_{12}+Y_{1} \\
    X_{2}=X_{21}+X_{22}+X_{23}+Y_{2},\\
    X_{3}=X_{31}+X_{32}+X_{33},
  \end{cases}
\end{equation}

onde $i=1,2,3$ denotar as atividades de produção para carvão, eletricidade e caminhões, respectivamente. Note que em (7), baseada na explicação dada acima, $X_{13}$ e $D_{3}$ são tomados como iguais a zero. 

A descrição da atividade de produção por meio de tais relações de insumo-produto inclui os seguintes pontos:

* A produção de uma mercadoria requer o uso de outras mercadorias como insumo. Mas, como foi o caso acima, uma atividade de produção não precisa usar todas as mercadorias disponíveis no sistema (o carvão não era um insumo na produção de caminhões).

* Uma mercadoria também pode ser demandada como produto final. No exemplo acima, é necessário carvão para aquecimento. Novamente, isso pode não ser verdade para todas as mercadorias. Os caminhões podem ser usados apenas para fins comerciais, e não como carros para as famílias.

O modelo parte de uma matriz de insumo-produto com dois setores do tipo abaixo:

<center>
![Modelo Insumo Produto Teórico (2x2)](IO.png){width=70%}
</center>

Agora vamos generalizar o exemplo dado acima para uma economia do $n$ setores em que cada setor produz apenas uma mercadoria $(X_{i})$ por uma técnica de produção. Em outras palavras, existe uma correspondência individual entre as mercadorias produzidas e as técnicas utilizadas em sua produção. Portanto, nessa estrutura simples, selecionar um setor é equivalente a identificar uma técnica de produção específica e uma mercadoria específica. No restante desta seção, esses termos serão usados de forma intercambiável.

Nessa economia, a produção de cada setor é usada por outros setores como insumo ou pelas famílias para a demanda final, ou seja, para consumo, investimento ou exportação. Na tabela acima, cada linha representa o uso do insumo do setor correspondente. As primeiras $n$ entardas fornecem o uso intersetorial da produção, ou seja, os valores alocados a outros setores como insumos, $(X_{ij})$. As entradas na coluna $n+1$ representam o uso final (demanda final) da produção (como consumo, investimento e exportação) do setor correspondente.

Se a oferta é igual à demanda de todas as mercadorias, pode-se escrever o seguinte conjunto de equações da tabela acima.

$$
\tag {8}
\begin{align*}
X_{i1}+X_{i2}+...+ X_{in}+Y_{i}=X_{1},  i=1,2,...,n.
\end{align*}
$$
Em cada equação, a soma dos primeiros $n$ elementos fornece a demanda interindustrial para a mercadoria correspondente e $Y_{i}$ é a demanda final.

Vamos definir a proporção de mercadoria $i$ utilizada na produção da mercadoria $j$ por

$$
\tag {9}
\begin{align*}
a_{ij}=\frac{X_{ij}}X_{j}
\end{align*}
$$

Então para dado conjunto de valores da produção $({X_1,X_2,...,X_n})$, a equação 8 pode ser reescrita como

$$
\tag {10}
\begin{align*}
\sum_{j=1}^na_{ij}X_{j}+Y{i}=X_{i}, i=1,2,...n. 
\end{align*}
$$

Assuma que a razão insumo por unidade produzida, isto é, $a_{ij}$, seja constante constante para todos os produtos. Então a equação (10) pode ser escrita em uma forma mais compacta usando vetores e matrizes, como

$$
\tag{11} 
\begin{align*} 
\mathbf{A}x+Y=x,
\end{align*}
$$

onde $A=(a_{ij})$ é uma matriz $n\times n$ de coeficientes de insumos, $x$ é um vetor $n\times 1$ de produção e $Y$ é um vetor $n\times 1$ de demanda final. 

Denote a j-ésima coluna da matriz $A$ por $a_{j}$,

$$a_{j}=\begin{bmatrix}
a_{1j}\\
a_{2j}\\
\vdots\\
a_{nj}
\end{bmatrix}
$$

Esse vetor representa o conjunto de insumos utilizados para produzir uma unidade de mercadoria $j$ e fornecerá os requisitos de insumos para produzir uma unidade de produto $j$. É convenientemente referida como a técnica de produção para $j$.

**2. O modelo de Leontief**

A tabela de transações entre setores é simplesmente uma fotografia das relações contábeis entre os setores. Para obter resultados relativos à estrutura produtiva dessa economia, são necessárias premissas mais fortes para conectar os níveis de produção à quantidade de insumos utilizados. Primeiro, a existência de relação entre insumos e produtos deve ser assumida. Isso pode ser feito descartando o caso irrealista da produção sem qualquer insumo de mercadorias.

**Hipótese 1** 
A produção de cada mercadoria requer o uso de pelo menos uma outra mercadoria como insumo.

Embora a suposição 1 seja suficiente para estabelecer uma relação entre os insumos da mercadoria e o produto, a natureza dessa relação não é especificada. No entanto, na análise da estrutura produtiva, é necessário especificar as propriedades dessa relação. Uma suposição historicamente importante e amplamente utilizada é a seguinte.

**Hipótese 2** A quantidade de um insumo necessário é proporcional ao nível da produção, ou seja, para qualquer insumo $i (i=1,..., n)$ 

$$\begin{align*}
\tag{12}
Insumo_{i} = a_{ij} \times produção_{j}, i,j=1,...,n,
\end{align*}
$$
onde $a_{ij}$ é assumida constante.

A suposição de linearidade, apesar de sua natureza simples e inócua, tem implicações muito mais abrangentes. Primeiro, como afirmado acima, essa suposição implica que os insumos necessários por unidade de produto permanecem invariáveis à medida que a escala da produção muda. Portanto, a suposição de linearidade implica *retornos constantes de escala* na produção. Em outras palavras, ao fazer essa suposição, são excluídas as tecnologias de produção que exibem *retornos decrescentes (ou crescentes) de escala*. Segundo, o fato de $a_{i}$ ser constante implica que a substituição não é permitida entre insumos. A tecnologia de produção permite que apenas uma técnica seja operada. Embora se possa supor que essa suposição se mantenha no curto prazo, é muito restritivo para representar as escolhas reais que uma unidade de produção enfrenta.

**Hipótese 3** (No joint production) Cada técnica de produção produz apenas uma produto.

Uma outra suposição implícita é a ausência de produção conjunta. Isso significa que cada técnica de produção produz apenas uma mercadoria. Na realidade, isso pode não ser o caso. Considere uma refinaria de petróleo. O refino de petróleo bruto é uma técnica que leva a vários derivados de petróleo, vários tipos de combustível e gás líquido. Nesse caso, a técnica de produção não pode ser rotulada referindo-se a apenas um produto. 


Nos termos da notação utilizada na tebela 1, a equação 12 pode reescrita como

$$\begin{align*}
\tag{13}
a_{ij} = \frac{X_{ij}}X_{i}.
\end{align*}
$$
Com $a_{ij} \ge 0$.

Sob as premissas (1)-(3), o sistema de produção caracterizado por (11) satisfaz as seguintes condições:

a. Todos os elementos da matriz $\mathbf A$ são negativos, isto é, $a_{ij} \ge 0$ e para todo $j$ existe algum $i$, tal que $a_{ij} \ge 0$;

b. Cada mercadoria é produzida apenas com uma técnica de produção, isto é, $\mathbf A$ é uma matriz quadrada;

c. Não há produção conjunta;

d. Há retornos constantes de escala para todas as técnicas de produção, ou seja, $\mathbf A$ permanece inalterada quando o vetor de produção $x$ muda.

Observe que as condições (a) e (b) significam simplesmente que $\mathbf A$ é uma matriz quadrada não negativa.

É bastante fácil mostrar que esse sistema possui uma solução única. Devido a (b) cada mercadoria pode ser caracterizada por sua técnica de produção, isto é, o vetor da coluna de $\mathbf A$ que corresponde à mercadoria em questão. Como as mercadorias são distintas, assim como suas técnicas de produção correspondentes. Além disso, também se pode assumir, no caso genérico, que as colunas de $\mathbf A$ são linearmente independentes. Portanto, $\mathbf A$ possui um *[rank](https://stattrek.com/matrix-algebra/matrix-rank.aspx)* completo, então existe e, por definição, é único. Então a solução para (11) é dada por 

$$
\begin{align*} 
x = \mathbf{(I-A)^{-1}}y,
\end{align*}
$$

ou seja, quando o vetor de demanda final é dado, é possível calcular os níveis de produção que permitem ao sistema satisfazê-lo.

É claro que o nível de produção tem que ter um valor não negativo. Uma unidade de produção pode decidir produzir a mercadoria em questão (ou seja, $x_{i} > 0$) ou pode interromper a produção (ou seja, $x_{i} = 0$). O nível de produção negativo não tem significado econômico nesse contexto. A equação (8) e as suposições que caracterizam a matriz $\mathbf A$ dos coeficientes de insumo garantem uma solução com significado econômico? A resposta é não. Para ver o porquê, considere o seguinte

*Exemplo 1* Considere uma economia de dois setores que opera sob as premissas (1) - (3). Seja dada a matriz dos coeficientes de insumo e o vetor de demanda final como

$$
\mathbf{A}= \begin{pmatrix}
  1.1 & 1.3\\
  0.3 & 0.2\\
    \end{pmatrix} \quad y=\begin{bmatrix} 
2\\
3\\
\end{bmatrix}
$$

Observe que $\mathbf A$ satisfaz a primeira condição (a) e a demanda final é positiva para ambas as mercadorias. 

Contudo. uma vez que

$$
\mathbf{(I-A)^{-1}} \approx \begin{pmatrix}
  -2.58 & -4.19\\
  -0.97 & -0.32\\
    \end{pmatrix}
$$    

o vetor de produção que satisfaz a demanda final fornecida é obtido como

$$
x \approx \begin{bmatrix} 
-17.73\\
-2.9\\
\end{bmatrix}
$$

isto é, níveis de produção negativos para ambas as mercadorias são obtidos, o que é claramente sem sentido do ponto de vista econômico.

É fácil mostrar que esse resultado é independente do vetor de demanda final. A estrutura da matriz $\mathbf A$ é tal que, para qualquer vetor de demanda final não negativo, não é possível obter níveis de produção total não negativos para ambas as mercadorias.

Usando a relação $\mathbf(I-A)$ $x=y$ podemos escrever

\begin{equation}
  \begin{cases}
    0.1x_{1}-1.3x_{2}=y_{1}\\
    -0.3x_{1}+0.4x_{2}=y_{2}.
  \end{cases}
\end{equation}

Somando essas duas equações lineares, pode-se obter

$$
\begin{align*} 
-0.2x_{1}-1.1x_{2}=y_{1}+y_{2} \geq 0.
\end{align*}
$$

É claro que, como o lado direito não é negativo, $x_{1}$ e $x_{2}$ não podem se,
simultaneamente, não são negativos, o que é inconsistente com o seu significado econômico.

Esse resultado demonstra que as suposições feitas até o momento não são suficientes para fornecer uma caracterização com pleno sentido econômico das condições de produção. Para encontrar a característica que falta, vejamos o exemplo dado acima. Neste exemplo, $a_{11}= 1.1$, isto é, requer $1.1$ unidades da mercadoria 1 para produzir uma unidade dessa mesma mercadoria. É claro que essa técnica de produção não é factível, uma vez que existe um "desperdício". Para descartar tais *wasteful production techniques*, devemos introduzir outra hipótese:

**Hipótese 4** (*non-wasteful production techniques*) \quad Cada técnica de produção é capaz de produzir mais de sua produção do que consome como insumo. Em termos dos coeficientes da matriz $\mathbf A$, essa suposição pode ser expressa como

$$
\begin{align*} 
a_{ii}<1
\end{align*} \quad \forall \quad i.
$$

Embora essa suposição deixe claro um ponto lógico, não descarta a possibilidade de um setor não ser capaz de fornecer a quantidade de produto necessária para produzir outras mercadorias e/ou satisfazer a demanda final.

Fica claro na equação (8) que a produção excedente (isto é, a quantidade de produção que não é consumida como insumo em sua própria produção) deve ser suficiente para satisfazer os requisitos de insumo do outro setor e a demanda final. Em outras palavras, o sistema de produção deve ser capaz de produzir pelo menos tanto quanto utiliza como insumos. O não desperdício, embora necessário, não é suficiente para garantir esse resultado. É necessária uma condição mais forte na estrutura produtiva. Isso é alcançado introduzindo uma nova suposição com base na seguinte definição.

**Definição 1** Uma matriz não-negativa $\mathbf A$ é chamada *produtiva* se existe $x>0$ tal que 

$$
\begin{align*} 
x>{\mathbf A}x
\end{align*}, 
$$

Se a matriz de coeficientes de insumos é produtiva, então o correspondente sistema produtivo $\mathbf A$$x+y=x$ também é chamado produtivo. 

**Hipótese 5** A matriz de coeficientes de insumos $\mathbf A$ é produtiva.

As cinco suposições feitas até agora são suficientes para caracterizar um sistema de produção capaz de fornecer resultados com sentido econômico.

**Definição 2** **(O modelo de Leontief)** Um sistema produtivo

$$
\tag {14}
\begin{align*} 
{\mathbf A}x+y=x
\end{align*}
$$
que satisfaz as seguintes hipóteses:

a. TOdos os elementos da matriz $\mathbf A$ são não-negativos, isto é, $a_{ij} \geq 0$ e para todo $j$ existe algum $i$ tal que $a_{ij}>0$;

b. Cada mercadoria é produzida apenas com uma técnica de produção, isto é, $\mathbf A$ é uma matriz quadrada;

c. Não há produção conjunta;

d. Há retornos constantes de escala para todas as técnicas de produção, isto é, $\mathbf A$ permanece constante quando ocorrem mudanças no vetor $x$;

e. $\mathbf A$ é denominada de produtiva.

**3. Existência de uma solução única e não-negativa ao sistema de Leontief**

Nessa seção vamos discutir e demonstrar como para qualquer $y>0$ é possível ter uma solução única e não-negativa ao sistema de Leontief. 

Para provar a existência e a unicidade de uma solução não negativa para um modelo de Leontief, quando a matriz de coeficientes de insumos é produtiva, provaremos alguns resultados matemáticos necessários para provar a proposição principal. Esses resultados são apresentados como lemas abaixo.

**Lemma 3.1** Deixe $x^{1} \quad e \quad x^{2}$ serem dois vetores tais que $x^{1} \ge  x^{2}$ e deixe $\mathbf A$ ser uma matriz não-negativa. Então temos que $\mathbf Ax^{1} \ge \mathbf Ax^{2}$.

*Prova* Deixe $x^{1}_{i}$ e $x^{2}_{i}$ serem os i-ésimos componentes dos vetores $x^{1}$ e $x^{2}$, respectivamente. Deixe $a_{ij}$ ser o elemento característico de $\mathbf A$. Então o i-ésimo elemento dos vetores $\mathbf Ax^{1} \ge \mathbf Ax^{2}$ pode ser escrito como

$$
\begin{align*} 
a_{i}x^{1}= \sum_{k=1}^n a_{ik}x_{k}^{1}\quad e \quad a_{i}x^{2}= \sum_{k=1}^n a_{ik}x_{k}^{2} \quad para \quad i=1,...,n,
\end{align*} 
$$
onde $a_{i}$ é o i-ésimo elemento da matriz $\mathbf A$ considerada como um vetor linha.

$$
\begin{align*} 
a_{i}x^{1} - a_{i}x^{2} = \sum_{k=1}^n a_{ik}(x^{1}-x^{2}) \quad \forall \quad i\\
ou \quad sinteticamente\\
\mathbf Ax^{1} \ge \mathbf Ax^{2}
\end{align*} 
$$

**Lemma 3.2** Se $\mathbf A$ é uma matriz produtiva, então todos elementos da matriz $\mathbf A^{s}$ convergem para $0$ quando $s \rightarrow \infty$.

*Prova* A definição 1 diz que

$$
\begin{align*} 
x>\mathbf A x \ge 0,
\end{align*}
$$

uma vez que $\mathbf A$ é uma matriz não-negativa. Então, existe um $\lambda$, $0<\lambda<1$, tal que

$$
\tag {15}
\begin{align*} 
\lambda x > \mathbf Ax \ge0.
\end{align*}
$$

Pré-multiplicando a equação 15 por $\mathbf A$ e usando o **Lemma 3.1**, temos

$$
\tag {16}
\begin{align*} 
\lambda (\mathbf Ax) > \mathbf A(\mathbf Ax) =\mathbf A^{2}x \ge0.
\end{align*}
$$

Por outro lado, pré-mutiplicando a equação 15 por $\lambda$, temos

$$
\tag {17}
\begin{align*} 
\lambda^{2} x > \lambda (\mathbf Ax) \ge 0.
\end{align*}
$$

As desiguldades 16 e 17, em conjunto fornecem

$$
\begin{align*} 
\lambda^{2} x > \mathbf A^{2}x \ge 0.
\end{align*}
$$

Similarmente a partir de 

$$
\begin{align*} 
\lambda^{s-1} x > \mathbf A^{s-1}x \ge 0.
\end{align*}
$$

obtemos

$$
\tag {18}
\begin{align*} 
\lambda^{s} x > \mathbf A^{s}x \ge 0.
\end{align*}
$$

Portanto, para qualquer matriz produtiva $\mathbf A$, a equação 18 é assegurada. Se $s \rightarrow \infty$, então $\lambda^{s} \rightarrow 0$, uma vez que $\lambda \in (0,1)$. Portanto, a partir da equação 18, temos

$$
\begin{align*} 
\lim_{s \to \infty} \mathbf A^{s}x = 0, 
\end{align*}
$$

ou

$$
\tag {19}
\begin{align*} 
\lim_{s \to \infty} \sum _{j=1}^{n} a_{ij}^{s} x_{j} = 0, \quad i=1,...n, 
\end{align*}
$$

com $a_{ij}^s$ sendo os elementos de $\mathbf A^{s}$. Uma vez que $x_{j} > 0$, a equação é assegurada somente se

$$
\begin{align*} 
\lim_{s \to \infty} a_{ij}^{s} = 0, \quad i,j = 1,...n.
\end{align*}
$$

**Lemma 3.3** Se $\mathbf A$ é uma matriz produtiva e se 

$$
\tag {20}
\begin{align*} 
x \ge \mathbf Ax
\end{align*}
$$

para algum $x$, então

$$
\begin{align*} 
x \ge 0.
\end{align*}
$$

*Prova* Multiplicando várias vezes a equação 20 sequencialmente por $s-1$ pela matriz $\mathbf A$ e usando o **Lemma 3.1**, obtemos

$$
\begin{align*} 
x \ge \mathbf Ax \ge \mathbf A^{2}x...\ge \mathbf A^{s}x
\end{align*}
$$

ou

$$
\tag {21}
\begin{align*} 
x \ge \mathbf A^{s}x
\end{align*}
$$

Quando $s \rightarrow \infty$, $\mathbf A^{s}x \rightarrow 0$, então a partir da equação 21 temos $x \ge 0 \quad$.

**Lemma 3.4** Se $\mathbf A$ é uma matriz produtiva, então $\mathbf{(I-A)}$ é uma matriz não-singular.

*Prova* Suponhamos que a matriz $\mathbf{(I-A)}$ seja singular, isto é, que o $det \mathbf {(I-A)} = 0$. Issp significa dizer que algumas colunas de $\mathbf{(I-A)}$ são [linearmente dependentes](https://en.wikipedia.org/wiki/Linear_independence). Então existe algum $x \neq 0$ tal que

$$
\tag {22}
\begin{align*} 
\mathbf{(I-A)}x = 0 \rightarrow x = \mathbf Ax.
\end{align*}
$$

A partir do **Lemma 3.3**, sabemos que $x \ge 0$. Agora considere o vetor $-x$. Como podemos constatar, esse vetor também satisfaz a equação 22, isto é,

$$
\begin{align*} 
\mathbf{(I-A)}(-x) = 0 .
\end{align*}
$$

Do **Lemma 3.3**, deve ser verdade que $-x \ge 0$. As desigualdades $-x \ge 0$ e $-x \le 0$ podem ser satisfeitas conjuntamente somente se $x=0$, o que contradiz que $x \neq 0$.

Os quatro lemmas permitem que agora tenhamos condições de mostrar o próximo teorema, o qual mostra que o modelo de Leontief tem uma solução única e não-negativa. 

**Teorema 5** Dado qualquer $y$, o sistema

$$
\tag {23}
\begin{align*} 
\mathbf{(I-A)}x = y
\end{align*}
$$

tem uma solução única e não-negativa se a matriz $\mathbf A$ é produtiva. 

*Prova* Deixe  $\mathbf{A}$ ser uma matriz produtiva. Suponha que $y \ge 0$ e considere a equação 23.

Pelo **Lemma 3.4**, $\mathbf (I-A)$ é não singular, uma vez que $\mathbf A$ é produtiva. Então a equação 23 tem uma solução única $\widetilde {x}$. Também é verdade que

$$
\begin{align*} 
\mathbf{(I-A)} \widetilde {x} = y
\end{align*}
$$

uma vez que $y \ge 0$. Então pelo **Lemma 3.3**, temos $\widetilde {x} \ge 0$. 

**Teorema 6** Deixe $\mathbf A$ ser uma matriz não-negativa. Então as seguintes condições são equivalentes:

i. $\mathbf A$ é produtiva;
ii. a matriz $\mathbf (I-A)$ existe e é não-negativa;
iii. todos os sucessivos menores principais de $B = \mathbf (I-A)$ são positivos. 

Na última condição, esses menores principai são:

$$
B_{1} = b_{11} > 0, \\
B_{2} =  \begin{bmatrix} 
b_{11} & b_{12}\\
b_{21} & b_{22}\\
\end{bmatrix} > 0, \\
... \\
B_{n} = B_{2} =  \begin{bmatrix} 
b_{11} \cdot \cdot \cdot b_{1n}\\
b_{21} \cdot \cdot \cdot b_{nn}\\
\end{bmatrix} > 0.
$$

A condição *iii* é chamada de condição Hawkins-Simon. A equivalência dessa condição *iii* e a condição *ii* de $\mathbf A$ ser produtiva é chamada de [Teorema Hawkins-Simon](https://www.jstor.org/stable/1905526?seq=1). 

*Prova* Um primeiro passo é provar que a condição *i* implica na condição *ii*. Inicialmente suponha que $\mathbf A$ seja produtiva. Então sabemos pelo **Lemma 3.4** que a matriz $\mathbf{(I-A)^{-1}}$ existe. Podemos definir a partir da série de expansão de Taylor 

$$
\begin{align*} 
\psi_{s} = I + A + A^{2} + A^{3} + \cdot\cdot\cdot + A^{s},
\end{align*}
$$

Então,

$$
\begin{align*} 
A\psi_{s} = A + A^{2} + A^{3} + \cdot\cdot\cdot + A^{s+1}.
\end{align*}
$$

Portanto,

$$
\begin{align*} 
(I-A)\psi_{s} = I - A^{s+1},
\end{align*}
$$

Tomando o limite de ambos os lados quando $s \rightarrow \infty$, temos

$$
\begin{align*} 
\lim_{s \rightarrow \infty} [(I-A) \psi_{s}] = I
\end{align*}
$$

uma vez que $A^{s+1} \rightarrow 0$, quando $s \rightarrow \infty$, de acordo com o **Lemma 3.2**. Portanto,

$$
\begin{align*} 
\psi_{s} = I + A + A^{2} + A^{3} + \cdot\cdot\cdot + A^{s} \rightarrow \mathbf{(I-A)^{-1}},
\end{align*}
$$

e, uma vez que $\mathbf{A} \ge 0$, temos que $\psi_{s} > 0$ e 

$$
\begin{align*} 
\mathbf{(I-A)^{-1}} \ge 0.
\end{align*}
$$

**4. Condições para obter uma solução positiva para o modelo Leontief**

Em termos práticos, se uma mercadoria não é demandada nem ofertada, ela poderá ser omitida da análise. Em outras palavras, uma solução economicamente significativa para um modelo de Leontief deve ser aquela que atribui níveis de produção positivos para todas as mercadorias, mesmo quando algumas mercadorias não são demandadas para uso final. Isso corresponde ao caso em que algumas mercadorias são usadas exclusivamente na produção (bens intermediários). A questão, então, é encontrar as condições que a matriz $\mathbf A$ deve satisfazer para obter níveis de produção positivos para todas as mercadorias. Obviamente, essas condições também devem ter uma interpretação econômica.

Antes de apresentar um resultado que garanta uma solução positiva para o modelo de Leontief, vamos nos concentrar na matriz $\mathbf (I-A)$ para extrair algumas de suas propriedades. Inicialmente note que

$$B=I-A=\begin{bmatrix}
1-a_{11} & -a_{12} & \cdots & -a_{1n}\\
-a_{21} & 1-a_{22} & \cdots & -a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
-a_{n1} & -a_{n2} & \dots & -a_{nn}
\end{bmatrix}
$$

e deixe $b_{ij} = 1-a_{ij}$ ser seus elementos característicos. Então

- (i) 

$$
\tag {24}
b_{ii} > 0 \quad [a_{ii} < 1],
$$

em termos econômicos, cada setor produz mais da sua produção do que consome como insumo (*non-wastefulness*).

- (ii) Qualquer mercadoria dentro do sistema pode ser usada como insumo, ou seja,

$$
b_{ij} \leq 0 \quad se \quad i \neq j.
$$

- (iii) Para qualquer coluna $j$ de $B$, podemos ter duas opções

$$
b_{jj} < 1 \quad [a_{jj} > 0]
$$

ou

$$
b_{ij} < 0 \quad [a_{ij} > 0] \quad para \quad algum \quad i,
$$

isto é, nenhum produto pode ser produzido sem usar pelo menos algum tipo de insumo. 

- (iv) As somas de linha de $B$ são não-negativas com pelo menos uma soma de linhas positiva, ou seja, o sistema é capaz de produzir excedentes para satisfazer a demanda final.








### Livros utilizados

### Vídeos para tópicos específicos

Excelente aula do professor do MIT, Gilbert Strang, discutindo matrizes inversas: 

<iframe width="560" height="315" src="https://www.youtube.com/embed/FX4C-JpTFgY" frameborder="0" allowfullscreen></iframe>